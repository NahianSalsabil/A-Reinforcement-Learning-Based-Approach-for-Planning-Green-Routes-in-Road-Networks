{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7812391,"sourceType":"datasetVersion","datasetId":4576011},{"sourceId":7985008,"sourceType":"datasetVersion","datasetId":4700237}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport networkx as nx\nimport numpy as np\nimport keras\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nimport pandas as pd\nimport networkx as nx\nimport math\nimport random\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom queue import PriorityQueue\nimport csv\nfrom datetime import datetime\nimport pytz\nimport os\nimport time\nimport copy\n\n\nnp.random.seed(42)\nnum_actions=8\n\nclass Env(object):\n    def __init__(self):\n        self.path = []\n        self.path_no_relations = []\n        self.path_relations = []\n        self.coords_path = []\n        self.curr_rewards = 0.0\n        self.curr_prob = 1.0\n        self.edge_rewards = {}\n        self.edge_transition_probs = {}\n        self.die = 0  # record how many times does the agent choose an invalid path\n        self.last_visited = {}\n        self.visited = {}\n        self.onlyreward=0.0\n        self.done = 0\n        self.steps = 0\n        self.path_length = 0.0\n\n    def copy(self):\n        o = Env.__new__(Env)\n        o.path = list(self.path)\n        o.path_relations = list(self.path_relations)\n        o.path_no_relations = list(self.path_no_relations)\n        o.onlyreward = self.onlyreward\n        o.coords_path = list(self.coords_path)\n        o.curr_rewards = self.curr_rewards\n        o.curr_prob = self.curr_prob\n        o.edge_rewards = {k: dict(v.items()) for k, v in self.edge_rewards.items()}\n        o.edge_transition_probs = {k: dict(v.items()) for k, v in self.edge_transition_probs.items()}\n        o.die = self.die  # record how many times does the agent choose an invalid path\n        o.last_visited = {k: tuple(v) for k, v in self.last_visited.items()}\n        o.visited = {k: set(v) for k, v in self.visited.items()}\n        o.done = self.done\n        o.steps = self.steps\n        o.path_length = self.path_length\n        return o\n\n    def __lt__(self, other):\n        # Define the comparison logic between instances of Env\n        return self.curr_prob < other.curr_prob\n\n    # def __lt__(self, other):\n    #     # Define the comparison logic between instances of Env\n    #     return self.curr_prob < other.curr_prob\n\n    def interact(self, state, action, rollout, prob):\n        done = 0  # Whether the episode has finished\n        cur = state[0]\n        dest = state[1]\n        next_node,green,aqi,noise,length=get_next_state(G,cur,action)\n        if(next_node==None):\n            print('the action is invalid ,no next state from here')\n        else:\n            edge_reward = calculate_combined_reward(aqi, green, noise,length)\n            self.path.append(str(action)+'->'+next_node) #keno use hocce buji nai\n            self.path_no_relations.append(next_node) #keno use hocce buji nai\n            self.coords_path.append((G.nodes[next_node]['latitude'],G.nodes[next_node]['longitude'])) #keno use hocce buji nai\n\n            # add action taken to dictionary of state-action pairs\n            self.last_visited[cur] = (action,next_node,len(self.path) - 1) #3rd param buji nai\n            if cur in self.visited:\n                self.visited[cur].add(action)\n            else:\n                self.visited[cur] = {action}\n\n            self.die = 0\n            if next_node == dest and not rollout:\n                done = 1\n                self.done = 1\n\n\n        # save above reward data for each node visited for future rewards\n        if cur in self.edge_rewards:\n            if next_node not in self.edge_rewards[cur]:\n                self.edge_rewards[cur][next_node] = (aqi,green,noise,length,edge_reward)\n        else:\n            self.edge_rewards[cur] = {next_node:(aqi,green,noise,length,edge_reward)}\n\n        # save probabilities for each node taken\n        if cur in self.edge_transition_probs:\n            if next_node not in self.edge_transition_probs[cur]:\n                self.edge_transition_probs[cur][next_node] = prob\n        else:\n            self.edge_transition_probs[cur] = {next_node: prob}\n\n        # print('in interact methdo: ')\n        # print('cur,nest: ',cur,next_node)\n        # print('probs: ',self.edge_transition_probs[cur][next_node])\n        next_state=(next_node,dest,self.die)\n        self.curr_rewards += edge_reward\n        self.curr_prob *= prob\n        self.steps += 1\n        self.onlyreward+=edge_reward\n        self.path_length += G[cur][next_node]['e_l']\n        # print('len: ',G[cur][next_node]['e_l'],cur,next_node,edge_reward)\n        # exit(0)\n        return edge_reward, next_state,G[cur][next_node]['e_l'], done\n\nclass PolicyNetwork(tf.keras.Model):\n    def __init__(self, num_actions,learning_rate=0.0005, seed=None):\n        super(PolicyNetwork, self).__init__()\n        self.initializer = keras.initializers.GlorotUniform(seed=seed)\n        self.dense1 =Dense(512, activation='relu',kernel_initializer=self.initializer)\n        self.dense2 =Dense(1024, activation='relu',kernel_initializer=self.initializer)\n        self.output_layer =Dense(num_actions, activation='softmax',kernel_initializer=self.initializer)\n        self.optimizer = Adam(learning_rate=learning_rate)\n\n    def call(self, state):\n        x = self.dense1(state)\n        x = self.dense2(x)\n        return self.output_layer(x)\n\n    def update(self, state, action):\n        with tf.GradientTape() as tape:\n            action_prob = self(state)\n            #print('action type in model: ',type(action),action)\n            action_mask = tf.one_hot(action, depth=num_actions)\n            picked_action_prob = tf.reduce_sum(action_prob * action_mask, axis=1)\n            loss = tf.reduce_sum(-tf.math.log(picked_action_prob ))+sum(tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.REGULARIZATION_LOSSES))\n        gradients = tape.gradient(loss, self.trainable_variables)\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n        return loss\n    \n    def reinforce_update(self, states, actions, rewards, gamma=0.4):\n        # Calculate the discounted returns for each time step\n        G = []\n        discounted_reward = 0\n        for r in rewards[::-1]:  # Reverse the rewards and iterate\n            discounted_reward = r + gamma * discounted_reward  # Apply discount factor\n            G.insert(0, discounted_reward)  # Insert at the beginning to maintain order\n\n        with tf.GradientTape() as tape:\n            loss = 0\n            for state, action, G_t in zip(states, actions, G):\n                state = tf.expand_dims(tf.convert_to_tensor(state, dtype=tf.float32), 0)  # Convert to tensor and add batch dimension\n                action_probs = self(state)[0]  # Get action probabilities from the policy network\n                # print('action_probs: ',action_probs)\n                action = tf.constant([action])\n                p_a = tf.gather(action_probs[0], action)\n                # print('g_t: ',G_t   )\n                # print('p_a: ',p_a)  \n                l=-tf.math.log(p_a)\n                # print('l: ',l)\n                loss += (l*G_t)  # Calculate the loss for this step\n\n        gradients = tape.gradient(loss, self.trainable_variables)  # Compute gradients\n        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))  # Apply gradients to update the model weights\n\n        return loss.numpy()\n\n    def predict(self, state):\n        action_prob = self(state, training=False)\n        return action_prob.numpy()\n\n\ndef calculate_initial_compass_bearing(pointA, pointB):\n    if (type(pointA) != tuple) or (type(pointB) != tuple):\n        raise TypeError(\"Only tuples are supported as arguments\")\n\n    lat1 = math.radians(pointA[0])\n    lat2 = math.radians(pointB[0])\n\n    diffLong = math.radians(pointB[1] - pointA[1])\n\n    x = math.sin(diffLong) * math.cos(lat2)\n    y = math.cos(lat1) * math.sin(lat2) - (math.sin(lat1)\n            * math.cos(lat2) * math.cos(diffLong))\n\n    initial_bearing = math.atan2(x, y)\n\n    initial_bearing = math.degrees(initial_bearing)\n    compass_bearing = (initial_bearing + 360) % 360\n\n    return compass_bearing\n\n\n\ndef id_angle(angle):\n    if angle <= 22.5 or angle > 337.5:\n        return 0\n    elif angle > 22.5 and angle <= 67.5:\n        return 1\n    elif angle > 67.5 and angle <= 112.5:\n        return 2\n    elif angle > 112.5 and angle <= 157.5:\n        return 3\n    elif angle > 157.5 and angle <= 202.5:\n        return 4\n    elif angle > 202.5 and angle <= 247.5:\n        return 5\n    elif angle > 247.5 and angle <= 292.5:\n        return 6\n    elif angle > 292.5 and angle <= 337.5:\n        return 7\n\n\n\ndef calculate_combined_reward(aqi, green, noise,p,p1):\n    a=b=c=0\n    if aqi >=7:\n        a = 100\n    elif aqi >=4:\n        a = 60\n    elif aqi >=0:\n        a = 30\n    if green >=7:\n        b = 100\n    elif green >=4:\n        b = 60\n    elif green >=0:\n        b = 30\n    if noise >=7:\n        c = 100\n    elif noise >=4:\n        c = 60\n    elif noise >=0:\n        c = 30\n    x=math.log(a)*aqi\n    y=math.log(b)*green\n    z=math.log(c)*noise\n    w=.000000001\n    # print('1st: ',x,y,z,x+y+z) \n    # print('plen: ',plen,norm)\n    # print('norm plean: ',plen/norm)\n    # print('w: ',w*plen)\n    fl=((y+x+z)*20)+w*(p/p1)\n    # print('fl: ',fl)\n    return fl\n\n\ndef get_next_state(graph,node,action_chosen):\n    next_chosen_node=None\n    next_nn = list(graph.neighbors(node))\n    actions=[]\n    for n in  next_nn:\n        action=id_angle(calculate_initial_compass_bearing((G.nodes[node]['latitude'],G.nodes[node]['longitude']),(G.nodes[n]['latitude'],G.nodes[n]['longitude'])))\n        if(action==action_chosen):\n            actions.append(n)\n    # print('len of same action: ',len(actions))\n    next_chosen_node=random.choice(actions)\n    green=graph[node][next_chosen_node]['e_g']\n    aqi=graph[node][next_chosen_node]['aqi']\n    len=graph[node][next_chosen_node]['e_l']\n    noise=graph[node][next_chosen_node]['e_n']\n    return next_chosen_node,green,aqi,noise,len\n\ndef state_embedding(state_idx,embed):\n    curr=embed[str(state_idx[0])]\n    targ=embed[str(state_idx[1])]\n    state=np.expand_dims(np.concatenate((np.asarray(curr), np.asarray(targ) - np.asarray(curr))), axis=0)\n    return state\n\ndef _normalize_probs(p):\n    if sum(p)>0:\n#         print('sum: ',sum(p))\n        p=[i+.000000001 for i in p]\n        \n        p/=sum(p)\n    else:\n        p = np.full_like(p, 1.0 / len(p))\n    # Check if the sum is close to zero\n   \n    return p\n\nG = pd.read_pickle(\"../input/normgraph/reversed_normalized_graph.gpickle\")\n\n\n\ndef create_modified_graph(original_graph):\n    modified_graph = original_graph.copy()\n    for edge in modified_graph.edges(data=True):\n        # Modify green view index attribute\n        edge[2]['e_g'] = 10 - edge[2]['e_g'] + 1\n    return modified_graph\n\n# Create the modified graph\nmodified_G = create_modified_graph(G)\n\ntraining_file='../input/normgraph/train_data_2000_10_150.txt'\n# training_file='train_data_2000_10_150.txt'\ntest_file='../input/normgraph2/test_data_2000_10_30.txt'\n# test_file='test_data_2000_10_30.txt'\nemd_file='../input/normgraph/node2vec_graph_num.emd'\n# emd_file='node2vec_graph_num.emd'\n\nwith open(emd_file,'r') as file:\n    data=file.readlines()\nnode_embeddings={}\nfor line in data:\n    node_embeddings[(line.split()[0])] = [float(x) for x in line.split()[1:]]\npolicy_network = PolicyNetwork(num_actions,seed=42)\nfailure=0\nr_cn=0\ng_cn=0\n\ndef choose_action_with_epsilon_greedy(action_probs, available_actions, epsilon=.1):\n    global r_cn, g_cn\n    if np.random.rand() < epsilon:  # With probability epsilon, choose a random action\n        r_cn += 1\n        return np.random.choice(available_actions)\n    else:\n        g_cn += 1\n        # Set the probabilities of unavailable actions to 0\n        adjusted_probs = np.zeros_like(action_probs)\n        for action_index in available_actions:\n            if action_index < len(action_probs):\n                adjusted_probs[action_index] = action_probs[action_index]\n\n        # Normalize the probabilities of available actions to sum to 1\n        if adjusted_probs.sum() > 0:\n            adjusted_probs /= adjusted_probs.sum()\n        else:\n            # If all adjusted_probs are 0 (unlikely but possible), treat it as a random choice among available actions\n            return np.random.choice(available_actions)\n\n        # Use np.random.choice to select an action based on the adjusted probabilities\n        return np.random.choice(len(action_probs), p=adjusted_probs)\n\n\ndef lat_long_to_cartesian(lat, lon, R=6371):  # R is the Earth's radius in kilometers\n    lat_rad = math.radians(lat)\n    lon_rad = math.radians(lon)\n\n    x = R * math.cos(lat_rad) * math.cos(lon_rad)\n    y = R * math.cos(lat_rad) * math.sin(lon_rad)\n    return x, y\n\ndef euclidean_distance(coord1, coord2):\n    return math.sqrt((coord1[0] - coord2[0]) ** 2 + (coord1[1] - coord2[1]) ** 2)\n\n\n\ndef run_pretrain(G,data):\n    episode=0\n    for row in data:\n        episode+=1\n        node=row.split(',')\n        src=int(node[0])\n        dest=int(node[1][:-1])\n        # print('source ,dest: for episode ',src,dest,episode)\n        dijkstra_path = nx.dijkstra_path(G,src,dest,weight='e_l')\n\n        with(tf.device('/device:GPU:0')):\n            for i in range(len(dijkstra_path)-1):\n                state = state_embedding((dijkstra_path[i],dest), node_embeddings)\n                action=id_angle(calculate_initial_compass_bearing((G.nodes[dijkstra_path[i]]['latitude'], G.nodes[dijkstra_path[i]]['longitude']),(G.nodes[dijkstra_path[i+1]]['latitude'], G.nodes[dijkstra_path[i+1]]['longitude'])))\n                policy_network.update(state,action)\n\n        if(episode%10==0):\n            print('epi done: ',episode)\n\n\ndef run_training_episodes(G,data):\n    global failure\n    path_model=[]\n    episode=0\n    print(\"hi\",len(data))\n\n    for row in data:\n        episode+=1\n        node=row.split(',')\n        src=int(node[0])\n        dest=int(node[1][:-1])\n        source_node=src\n        destination_node=dest\n        # print(type(source_node),type(destination_node))\n        # print('source ,dest: for episode ',source_node,destination_node,episode)\n        shortest_path_length = nx.shortest_path_length(G, source_node, destination_node, weight='e_l')\n\n        best_path=None\n        ga=None\n        gn=None\n        gg=None\n\n        # print('dijkstra path: ',np.max(aqi_dijkstra),np.min(green_dijkstra),np.max(noise_dijkstra))\n        # s_path = nx.shortest_path(G,source_node,destination_node,weight='e_l')\n        num_of_paths=0\n        for j in range(1500):\n            source_node=src\n            destination_node=dest\n            state_idx=[source_node,destination_node]\n            visited = [False] * 6000\n            visited[source_node] = True\n            done=0\n            traversed_nodes=[source_node]\n            p_len=0\n            for _ in range(6000):\n                cur_state=state_embedding(state_idx,node_embeddings)\n\n                action_probs = policy_network.predict(cur_state)\n\n                neighbor_nodes = list(G.neighbors(state_idx[0]))\n                available_actions=[]\n                available_neighbour_nodes=[]\n                for n in  neighbor_nodes:\n                    action=id_angle(calculate_initial_compass_bearing((G.nodes[state_idx[0]]['latitude'],G.nodes[state_idx[0]]['longitude']),(G.nodes[n]['latitude'],G.nodes[n]['longitude'])))\n                    # don't choose an action that has been chosen before in the same state\n                    if visited[n] == False:\n                        available_actions.append(action)\n                        available_neighbour_nodes.append(n)\n\n                if len(available_actions)==0:\n                    # print('no available actions')\n                    break\n\n\n                action_probs_avail = []\n                choices_idx = []\n                for c in available_actions:\n                    choices_idx.append(c)\n                    action_probs_avail.append(action_probs[0][c])\n                norm_action_probs = _normalize_probs(action_probs_avail)\n                \n                if np.random.random() < 0.1:\n                    # With 10% probability, choose a random action\n                    action_chosen = np.random.choice(available_actions)\n                else:\n                    # Choose an action based on the provided probability distribution\n                    action_chosen = int(np.random.choice(available_actions, p=norm_action_probs))\n\n                ind=choices_idx.index(action_chosen)\n                chosen_neighbor_node = available_neighbour_nodes[ind]\n                traversed_nodes.append(chosen_neighbor_node)\n\n                visited[chosen_neighbor_node] = True\n                p_len+=G[state_idx[0]][chosen_neighbor_node]['e_l']\n\n                eudis=euclidean_distance(lat_long_to_cartesian(G.nodes[dest]['latitude'], G.nodes[dest]['longitude']),lat_long_to_cartesian(G.nodes[chosen_neighbor_node]['latitude'], G.nodes[chosen_neighbor_node]['longitude']))\n                \n                if(p_len+eudis>2*shortest_path_length):\n                    # print('no path within twice the shortest path length')\n                    break\n                \n                if chosen_neighbor_node == destination_node:\n                    done=1\n                    break\n\n                state_idx=[chosen_neighbor_node,destination_node]\n\n            if(done==1):\n                # print(f'Successfully reach destination in retrain number {j+1} in episode {episode} need step: {i+1}')\n                num_of_paths+=1\n\n                states = []\n                actions = []\n                rewards = []\n                rl_aqi=[]\n                rl_noise=[]\n                rl_green=[]\n\n                for i in range(len(traversed_nodes) - 1):\n                    state_idx=[traversed_nodes[i], destination_node]\n        #             print('whew wrong: ',state_idx[0],state_idx[1])\n                    state = state_embedding(state_idx, node_embeddings)\n\n                    action = id_angle(calculate_initial_compass_bearing(\n                        (G.nodes[traversed_nodes[i]]['latitude'], G.nodes[traversed_nodes[i]]['longitude']),\n                        (G.nodes[traversed_nodes[i + 1]]['latitude'], G.nodes[traversed_nodes[i + 1]]['longitude'])\n                    ))\n                    green=G[traversed_nodes[i]][traversed_nodes[i + 1]]['e_g']\n                    aqi=G[traversed_nodes[i]][traversed_nodes[i + 1]]['aqi']\n                    length=G[traversed_nodes[i]][traversed_nodes[i + 1]]['e_l']\n                    noise=G[traversed_nodes[i]][traversed_nodes[i + 1]]['e_n']\n                    rl_aqi.append(aqi)\n                    rl_noise.append(noise)\n                    rl_green.append(green)\n                    reward = calculate_combined_reward(aqi, green, noise,shortest_path_length,p_len)\n                    states.append(state)\n                    actions.append(action)\n                    rewards.append(reward)\n\n\n                policy_network.reinforce_update(states, actions, rewards)\n\n                # print('success in episode: ',j)\n                # print('our path: ',np.max(rl_aqi),np.min(rl_green),np.max(rl_noise))\n                if ga==None:\n                    ga=np.min(rl_aqi)\n                else:\n                    if(np.min(rl_aqi)>ga):\n                        ga=np.min(rl_aqi)\n                        best_path=traversed_nodes\n                if gn==None:\n                    gn=np.min(rl_noise)\n                else:\n                    if(np.min(rl_noise)>gn):\n                        gn=np.min(rl_noise)\n                        best_path=traversed_nodes\n                if gg==None:\n                    gg=np.min(rl_green)\n                else:\n                    if(np.min(rl_green)>gg):\n                        gg=np.min(rl_green)\n                        best_path=traversed_nodes\n            else:\n                # print('ist sn: ',s_n,cnt)\n                pass\n\n        if(best_path!=None):\n            path_model.append(best_path)\n        else:\n            failure+=1\n        print('num of paths: epi',num_of_paths,episode)\n\n    with open('exp12_train.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(path_model)\n\n        \nwith open(training_file,'r') as file:\n        train_data=file.readlines()\n# run_pretrain(G,train_data)\n\nrun_training_episodes(G,train_data)\nprint('fail:',failure,r_cn,g_cn)\n\nimport heapq\nimport time\n\n\n\ndef beam_search(graph, start, end, k=5, step_limit=6000):\n    start_time = time.perf_counter_ns()  # Record the start time\n\n    paths = [(1, [start], {start})]  # (cost, path, visited)\n    final_paths = []\n    steps = 0\n\n    while paths and len(final_paths) < 5 and steps < step_limit:\n        new_paths = []\n        for cost, path, visited in paths:\n            current = path[-1]\n            if current == end:\n                final_paths.append((cost, path))\n                if len(final_paths) == 5:\n                    break\n                continue\n\n            if current in graph:\n                \n                cur_state=state_embedding((current,end),node_embeddings)\n\n                action_probs = policy_network.predict(cur_state)\n\n                neighbor_nodes = list(G.neighbors(current))\n                available_actions=[]\n                available_neighbour_nodes=[]\n                available_probs=[]\n                for n in  neighbor_nodes:\n                    action=id_angle(calculate_initial_compass_bearing((G.nodes[current]['latitude'],G.nodes[current]['longitude']),(G.nodes[n]['latitude'],G.nodes[n]['longitude'])))\n                    # don't choose an action that has been chosen before in the same state\n  \n                    available_actions.append(action)\n                    available_neighbour_nodes.append(n)\n                    available_probs.append(action_probs[0][action])\n\n                if len(available_actions)==0:\n#                     print('no available actions')\n                    break\n                \n                available_probs = _normalize_probs(available_probs)\n\n                for next_node in available_neighbour_nodes:\n                    if next_node not in visited:\n                        new_visited = visited.copy()\n                        new_visited.add(next_node)\n                        new_path = path + [next_node]\n                        new_cost = cost+ (-math.log(available_probs[available_neighbour_nodes.index(next_node)]))\n                        new_paths.append((new_cost, new_path, new_visited))\n\n        paths = heapq.nlargest(k, new_paths, key=lambda x: x[0])\n        steps += 1\n\n    end_time = time.perf_counter_ns()  # Record the end time\n\n    if final_paths:\n        best_path = max(final_paths, key=lambda x: x[0])[1]\n    else:\n        best_path = None\n\n    execution_time = (end_time - start_time)/1000000  # Calculate the execution time\n\n    return best_path, execution_time\n\n\nwith open(test_file, 'r') as file:\n    test_data = file.readlines()\n\nepisode=0\ntime_taken_arr=[]\nsucess_test_Rate=0\ntest_paths=[]\nfor row in test_data:\n    episode+=1\n    node=row.split(',')\n    src=int(node[0])\n    dest=int(node[1][:-1])\n\n    path, time_taken = beam_search(G, src,dest)\n    print(\"Path found:\", path)\n    print(\"Time taken (seconds):\", time_taken)\n    if path is not None:\n        sucess_test_Rate+=1\n        test_paths.append(path)\n        time_taken_arr.append(time_taken)\n\nprint(\"Average time taken (seconds):\", sum(time_taken_arr) / len(time_taken_arr))\nprint(\"Success rate:\", sucess_test_Rate,30-sucess_test_Rate,sucess_test_Rate / len(test_data))\n\nwith open('exp12_test.csv', 'w', newline='') as file:\n        writer = csv.writer(file)\n        writer.writerows(test_paths)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-13T12:59:50.325714Z","iopub.execute_input":"2024-06-13T12:59:50.326067Z","iopub.status.idle":"2024-06-13T13:00:32.958757Z","shell.execute_reply.started":"2024-06-13T12:59:50.326038Z","shell.execute_reply":"2024-06-13T13:00:32.956854Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-06-13 12:59:52.517291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-06-13 12:59:52.517449: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-06-13 12:59:52.662969: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"hi 150\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/251988574.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[0;31m# run_pretrain(G,train_data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m \u001b[0mrun_training_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fail:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfailure\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr_cn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg_cn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mheapq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/251988574.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(G, data)\u001b[0m\n\u001b[1;32m    493\u001b[0m                     \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m                     \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m                 \u001b[0mpolicy_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreinforce_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m                 \u001b[0;31m# print('success in episode: ',j)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m                 \u001b[0;31m# print('our path: ',np.max(rl_aqi),np.min(rl_green),np.max(rl_noise))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/251988574.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, states, actions, rewards, gamma)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0ml\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;31m# print('l: ',l)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mG_t\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Calculate the loss for this step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mgradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Apply gradients to update the model weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1062\u001b[0m               output_gradients))\n\u001b[1;32m   1063\u001b[0m       output_gradients = [None if x is None else ops.convert_to_tensor(x)\n\u001b[1;32m   1064\u001b[0m                           for x in output_gradients]\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     63\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     raise ValueError(\n\u001b[1;32m     65\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     68\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/math_grad.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1832\u001b[0m   \u001b[0madj_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"adj_y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madj_x\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madj_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1836\u001b[0;31m       \u001b[0mgrad_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjoint_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1837\u001b[0m       \u001b[0mgrad_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjoint_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1839\u001b[0m       \u001b[0mgrad_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjoint_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjoint_b\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_auto_dtype_conversion_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     \u001b[0mbound_arguments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mbound_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbound_arguments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1258\u001b[0m       \u001b[0;31m# Fallback dispatch system (dispatch v1):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1261\u001b[0;31m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1262\u001b[0m         \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1263\u001b[0m         \u001b[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1264\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop_dispatch_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0muse_batch_matmul_v3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3792\u001b[0m         return gen_math_ops.batch_mat_mul_v3(\n\u001b[1;32m   3793\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001b[1;32m   3794\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3795\u001b[0;31m         return gen_math_ops.batch_mat_mul_v2(\n\u001b[0m\u001b[1;32m   3796\u001b[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, name=name)\n\u001b[1;32m   3797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3798\u001b[0m     \u001b[0;31m# Neither matmul nor sparse_matmul support adjoint, so we conjugate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(x, y, adj_x, adj_y, name)\u001b[0m\n\u001b[1;32m   1618\u001b[0m         _ctx, \"BatchMatMulV2\", name, x, y, \"adj_x\", adj_x, \"adj_y\", adj_y)\n\u001b[1;32m   1619\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1621\u001b[0m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1623\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1625\u001b[0m       return batch_mat_mul_v2_eager_fallback(\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]}]}